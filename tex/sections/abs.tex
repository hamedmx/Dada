\begin{abstract}
%In deep neural networks, the outcome of the classification task is determined based on the highest predictive probability of the target labels. The outcome probabilities of unselected target labels are either dismissed or considered as the uncertainty associated with the prediction. In machine learning research, this associated uncertainty is sometimes used as the only indicator of the model trustworthiness. 

In this paper, we 
%argue that this uncertainty is not adequately expressive and interpretable for trust quantification and 
propose a method to incorporate the uncertainty associated with the input data of a deep neural network and propagate this uncertainty throughout the training phase using subjective logic.    
%into the  and propagate the into nd model parameters in the trustworthiness of the learning model. According to the impact of training data distributional properties on the model performance, 
We represent intrinsic statistical properties of input data as subjective opinions and through the propagation of input data opinions, we    
%and propagate  attributes to initialize the input data opinions to be propagated through a DNN using Subjective Logic as a tool, to 
quantify the total opinion of the learning model that works as a trust metric for the outcome of the DNN. In our experiments, we demonstrate that statistical belief and uncertainty associated with the input data can impact the overall trustworthiness of a DNN model. 
%We also provide an analytical tool to study the trade-offs between projected trust probability and overall model accuracy. 
We observe that belief and projected probability of the final opinion of the model is increasing while uncertainty is decreasing.  
\end{abstract}

\begin{IEEEkeywords}
Deep Neural Network; Subjective Logic; Opinions; Uncertainty; Trustworthiness; Machine Learning;
\end{IEEEkeywords}

  
% In deep neural networks, the outcome of the classification task is determined based on the highest predictive probability of the target labels. The outcome probabilities of unselected target labels are either dismissed or considered as the uncertainty associated with the prediction. In machine learning research, this associated uncertainty is sometimes used as the only indicator of the model trustworthiness. In this research, we argue that this uncertainty is not adequately expressive and interpretable for trust quantification and propose a novel approach to incorporate uncertainty associated with datapoints and model parameters in the trust metric of the learning model. We leverage Subjective Logic as a tool to propagate and measure the uncertainty in DNNs, with an aim of quantifying total uncertainty in a decentralized federated learning system. We specifically focus on the federated learning application domain as trust in this domain is a make or break factor in aggregated model outcome. We demonstrate that statistical uncertainty (e.g., based on explained variance or eigenvalues) associated with the datapoints and belief or disbelief on model parameters can impact the overall trustworthiness of an FL model. We also provide an analytical tool to study the trade-offs between projected trust probability and overall model accuracy. This approach is particularly helpful in FL context when a culprit node needs to be identified in an objective and timely manner. 


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% One of the trust criteria which is not well addressed in deep neural networks is the amount of uncertainty. Uncertainty in ML applications is a first-order uncertainty (Aleatory, Epistemic, or the hybrid version) to quantify a degree of uncertainty which is not expressive and will be applied in some specific circumstances. To be more expressive, there is a need to a second-order uncertainty other than prior uncertainties, to be applied and fused in a higher level in terms of considering other properties. We leverage Subjective Logic as a tool and inject it to deep neural networks in order to propagate and measure the uncertainty in a single DNN, with an aim of propagating and quantifying total uncertainty in a decentralized federated learning system. Furthermore, using subjective logic in our proposed approach is fully beneficial and adequately flexible in different DNN architectures since it can act as an add-on in a ML system by being applied along with the training process in each DNN, therefore, it has no significant additional computational burden. It is also compatible with the state-of-the-art methods of communication amongst different DNNs. The objective of our research is to develop a novel method for propagating uncertainty associated with training dataset as subjective opinions through a DNN model leading to the total model opinion containing total model uncertainty. Thus, we exploit some statistical and conceptual method to quantify and initialize data uncertainty and opinions to be propagated based on the nature of input data features. During training process, the model updates its parameters' opinions and outputs its total opinion. Each feature or label opinion individually participates in training process. Therefore, for opinion propagation through a network, the opinions of data features and labels are separately initialized. 
% We evaluate our proposed method using two real-world datasets, SUSY and KDD CUP 99, containing numerical and categorical features which are considerably different in terms of many statistical, natural, inferential, and domain aspects. Based on our evaluation, we expect a DNN model with higher accuracy indicates lower uncertainty. Therefore, we observe that by decreasing loss and increasing accuracy during our training epochs which is the sign of successful learning, the amount of belief is increasing while our model uncertainty is decreasing significantly. We show that the belief and uncertainty will be improved in a DNN model based on its loss function optimization during training process. Moreover, we indicate the impact of successful training and loss optimization on the total opinion and its determinants in the learning model. We also demonstrate the impact of different statistical or conceptual factors of data opinion initialization over total model opinion’s determinants like belief and uncertainty in a way that each opinion determinant is relatively improved during training epochs. As a future work, we are currently working on propagating data and model opinions through a decentralized federated settings in which we can train different client models over their private data without sharing confidential data information to achieve an improved global model. We intend to quantify the total uncertainty of a federated network based on propagated clients’ opinions over a specific amount of training rounds. Therefore, we utilize our approach in each communication, aggregation, and propagation activities in federated learning without a significant computational cost.


