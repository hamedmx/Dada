

%\section{Data Opinion Propagation Model}
\section{Current Opinion Propagation Models}
\label{method}

The following section provides the necessary preliminaries on the background of uncertainty and subjective logic as an approach leveraged to propagate data opinions in a DNN. Then, we provide subjective logic concepts and a summary of the current method of opinion propagation proposed by~\cite{hope}. Finally, we propose our method for the data opinion initialization and optimization process.
 
\subsection{Preliminaries and Background}
\label{background}

% \subsection{Single Deep Neural Network}
% A Deep Neural Network (DNN) is a computational network that can be considered as a function $\mathcal{F}_\Theta:~\mathcal{D} \rightarrow \hat{Y}$ in which $\mathcal{D}$ is a set of input data fed into the network, $\Theta$ is a set of network parameters (weights and biases), and $\hat{Y}$ is the output set of the approximated values $\hat{y} \in \hat{Y}$. Moreover, the DNN model $\mathcal{M}_{\Theta}$ aims to be trained based on its input data (a finite set of training data) denoted by $\mathcal{D}$, according to the function~$\mathcal{Q}:~\mathcal{D}~\rightarrow~Y$ such that, 
% \[
% \mathcal{Q} = \{\ (d_{i},y_{i})\ |\ i \in \mathbb{N}^{[1,m]}\ \}\ ,
% \]
% where $i$ is a data point with $d_{i}$ and $y_{i}$ representing features and target (label), respectively and $m$ is the number of participating data points (data samples) in the training dataset.

% In the DNN model $\mathcal{M}_{\Theta}$, we intend to optimize a loss function in the form of $\mathcal{L}(\mathcal{D},\Theta;Y)$. This optimization is performed by minimizing the average of each data sample $i$'s loss value with respect to the parameter set of $\Theta$ defined as, 
% \begin{equation}
% \min_{\Theta}\ \mathcal{L}(\mathcal{D},\Theta;Y) = \frac{1}{m}\ \sum_{i=1}^{m} loss(d_{i},\Theta;y_{i})\ ,
% \label{loss}
% \end{equation}
% where $loss(d_{i},\Theta;y_{i})$ is the loss function of the single data sample $i$ in the training dataset.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% In deep learning, the layers of a DNN are either inner (hidden) layers, logits, or softmax. The number of inner layers could be different from one DNN to another depending on the application, input data, number of data features. However, the latter two layers are common among all DNNs in classification tasks. The logit layer is the second last layer and provides a set of $\hat{y} \in \hat{Y}$ generated by the last hidden layer of the DNN model such that $|\hat{Y}|$ is the number of labels. The softmax layer is the last layer that normalizes the output of logit layer into the probability vector $P_{\hat{Y}}$ as the final output of $\mathcal{M}_{\Theta}$. Softmax function generates the set of probabilities $P_\hat{Y}$ associated with each logit $\hat{y}$ for each target label $y_{l}$. Each $p_{\hat{y}} \in P_{\hat{Y}}$ is the probability of the target label $l \in L$ being selected while classifying an input data $\mathcal{D}_i$ in $\mathcal{M}_{\Theta}$. Formally, in the softmax layer, each neuron $i$ corresponding to the label $l$, computes the probability $p_{\hat{y}}$ of the output probability vector $P_{\hat{Y}}$ as,
% \begin{equation}
%     P_{\hat{Y}}=softmax(\hat{Y})=\{\ \frac{e^{\hat{y}_{i}}}{\sum_{i=1}^{n_l} e^{\hat{y}_{i}}}\ \}_{i \in \mathbb{N}^{[1,|\hat{Y}|]}}\ ,
% \label{softmax}
% \end{equation}
 
% Accuracy as an important indicator of the model performance, is obtained based on the set of logits $\hat{Y}$ of the DNN model $\mathcal{M}_{\Theta}$.

%%%%%%%%%%% IMPORTANT %%%%%%%%%%%%%%%%%
% As it can be seen in the Fig.~\ref{dfl},
% \begin{figure}[H]
% 	\centering
% 	\includegraphics[scale=0.4]{figures/rp.png}
% 	\caption{An example of decentralized federated settings: The temporary head will be determined and selected to aggregate the updates of the participating models using local differential privacy in each training round $t$ in order to quantify Trust and uncertainty in this settings based on subjective logic.}
% 	\label{dfl}
% \end{figure}

%\vspace {.3cm}
\noindent
\textbf{Uncertainty.}
In the DNN classification task (see Appendix~\ref{dnn_app}), the outputs of the model are the softmax probabilities $p_{\hat{y}} \in P_{\hat{Y}}$, and are considered as the confidence of the DNN model in selecting the target labels for the training data $\mathcal{D}$~\cite{deeplearning,confidence}. 
% For example, in the task of classifying the data sample $\mathcal{D}_i$ into one of the three labels \textit{\{Airplane, Car, Truck\}} with $P_{\hat{Y}}=\{0.02,\ 0.8,\ 0.18\}$, the label $l=$ \textit{`Car'} will be selected for $\mathcal{D}_i$ with the highest confidence (probability) of 0.8. 
% However, there are informative confidence or probability values for other labels which indicates how much confidence the model $\mathcal{M}_{\Theta}$ has for each label regardless of $d_i$ being correctly classified or misclassified. 
%To address the first-order uncertainty,

The total uncertainty of a DNN model is generally defined as the lack of confidence in classifying each data sample $d \in \mathcal{D}$. This uncertainty denoted by $U_{d}$, is calculated using the entropy of the probability vector $P_{\hat{Y}}$ as,
\begin{equation}
U_{d}(P_{\hat{Y}}) = - \sum_{\hat{y} \in \hat{Y}} p_{\hat{y}}\log p_{\hat{y}}\ .
\label{entropy}
\end{equation}

There are two types of uncertainty that can be used in deep learning: first-order and second-order uncertainty. %Traditional probability (probabilistic logic) represents first-order probability that develops information about the statements and expresses the relationship between those statements. Each statement is a meaningful declarative sentence that is evaluated to be true or false. 
$U_{d}$ is a first-order type uncertainty, quantified by the first-order probability of softmax function~\cite{uncer4,uncer5}. 
However, first-order probabilistic logic is limited in capturing ownership of the assigned or computed probability. For example, the probability extracted from softmax function may change, when the modeler or any other sources in a DNN (e.g., a neuron) has a certain opinion about the model outcome probability (or model performance) generated by the softmax function. To capture this notion, second-order uncertainty is defined as the probability  %may be understood as the probability 
that the first-order uncertainty has a certain value. This value can be interpreted as the value that would be assigned if certain information about the first-order probability were available, including opinions of a modeler or any other sources. In addition, we may encounter situations where the owner or the source cannot produce probability for the first-order uncertainty or where multiple owners or sources have different opinions about the uncertainty. 




%has three main limitations 1) in capturing ownership of first-order probabilities. For example, first-order probabilities like softmax probability does not have any owner (or source) to hold and present the probability. 2) the situation when an owner cannot produce probability for a random variable is not considered (no uncertainty is considered) 3) the situation where multiple owners or sources have different opinions about a statement, is not captured.

%A second-order uncertainty captures the probability  
%may be understood as the probability 
%that the first order uncertainty has a certain value. This value can be interpreted as the value that would be assigned if certain information about the first order probability were available, including opinions of a modeler or any other sources. 



%quantifies the probability of random variables 
%containing individual statements

% {\color {red} RS: WHY only ? what is statement?}

%%%% important %%%%
% We can use averaging to calculate total training uncertainty and total training confidence in each target label $l$ over all training data. Moreover, the maximum uncertainty resulted from the entropy equation is gained by uniformly assigning probabilities over all the classes (target labels).
%%%%
% The aforementioned uncertainty is captured and decomposed to two different types in deep learning~\cite{uncer-rev}: \textit{Aleatoric Uncertainty} (or data uncertainty) and \textit{Epistemic Uncertainty} (or model uncertainty). Aleatoric uncertainty comes from the noise inherently exists in input data and associated with information loss in representing real-world data samples. Therefore, this uncertainty cannot be reduced by training over adequate amount of data or improving training procedure. Aleatoric uncertainty is further divided into two different sub-categories:
% \begin{enumerate}
%     \item \textit{Heteroscedastic uncertainty} (data-dependant)  which is dependent to the input data and is predicted as a model output.
%     \item \textit{Homoscedastic uncertainty} (task-dependant) which is not dependant to the input data and stays constant for all input data and differs among various tasks.
% \end{enumerate}
% }
% Epistemic uncertainty is the uncertainty rooted in the model parameters and caused by insufficient coverage of training data, errors in training process, errors caused by unknown data or errors in model architecture. Thus, epistemic uncertainty can be reduced by training the DNN over an adequate amount of data, improving model structure, or rectifying training process~\cite{uncer-rev}. 

% Probabilistic logic (traditional probability) indicates first-order uncertainty using first-order probability. First-order probability quantifies the probability of only random variables containing individual statements. 
% Both aleatory and epistemic uncertainty present first-order uncertainty~\cite{uncer-rev}. 
% For example, $x$ is an individual statement of a binary random variable $\mathbb{X}=\{x,\overline{x}\}$ like flipping an unfair coin. When $x=head$, the probability of obtaining $head$ in flipping this coin can be $P(x)=0.6$ which demonstrates an amount of uncertainty in obtaining the individual statement $x \in \mathbb{X}$.

%A second-order probability Q(P) may be understood as the probability that the true probability of something has the value P. “True” may be interpreted as the value that would be assigned if certain information were available, including information from reflection, calculation, other people, or ordinary evidenc


%First-order probabilistic logic has three main limitations in capturing uncertainity originated from the ownership of opinions ~\cite{sl}: 1) the amount of the analyst's confidence cannot be captured, 2) the situation when the analyst cannot produce probabilities for some input arguments, is not considered, and 3) the situation when multiple sources have various certain opinions (beliefs) about the same statement. Subjective logic as a tool prevents the above limitations to extend probabilistic logic using second-order logic considering uncertainty about probabilities as well as beliefs about a statement~\cite{sl}. More specifically, second-order probability computes the probability over functions or sets of individuals as well as the very individuals. Second-order uncertainty is represented in the form of the Beta or Dirichlet probability density functions (PDF) over first-order probabilities. 
% {\color{red}
% RS: HAS TO BE REWRITTEN- many problems: FIRST you should say there are other types of uncertainty, THEN you should define what is statement and their truth value. THEN you should say there are is second order... to capture these type of uncertainty, THEN define second o u} 
% We need to resolve the limitations of using probabilistic logic in order to be used in propagating the uncertainty in a DNN. 

\vspace {.3cm}
\noindent
\textbf{Subjective Logic}.
Subjective Logic extends probabilistic logic by including uncertainty about first-order probabilities and subjective belief ownership~\cite{sl}. Subjective logic utilizes arguments called \emph{Opinion} expressing separate beliefs about the truth of statements under an uncertainty mass as a second-order uncertainty. Although uncertainty generally means the lack of confidence, in subjective logic, uncertainty is the vacuity of evidence. This opinion representation is equivalent to the Dirichlet distribution and shows ownership of an opinion. 
Furthermore, subjective opinion ownership is closely related to trust since when there are different opinions about the same first-order probability, then a source can determine trust levels in other different sources of opinions before being integrated in a reasoning model~\cite{sl}.

% There are two main definitions we use in this research:

% \noindent\textbf{Definition 1:} A DNN Component $v$ is either an input data feature, a parameter (weight or bias), a neuron, or the entire neural network.{\color {red} RS: THE COMPONENTS ARe obviously not disjoint?!}
%DNN model 
%which containing data and participating in computational operations, 
%are considered as components of a neural network model. % $\hfill \blacksquare$

% {\color {red} RS: BIG GAP!! in introducing subjective logic concepts.  OPINION needs to be defined, then none of $b_v, d_v, u_v$ are defined.  the whole things also doesn't  belong here - perhaps should be part of your methodology  }

% {\color {red} RS: REWRITE - VERY POORLY WRITTEN \noindent\textbf{Definition 2:} Trustworthiness of a DNN component $v$ is defined as the projected probability of an opinion.  Since DNN components impact the model performance in each training epoch, following subjective logic, each DNN component $v$ has a binomial opinion $O_v=\{b_{v},d_{v},u_{v},a_{v}\}$ about the DNN model performance 
% %a subjective concept which is the model performance (accuracy) 
% such that $b_v + d_v + u_v = 1$. Moreover, the projected probability of an opinion $PP_v = b_v + u_v.a_v$ is considered as the trustworthiness of the component's opinion since it is an expected probability of all the opinion's determinants~\cite{sl} (see Appendix~\ref{sl_app} for more details).}  

\subsection {Current Method of Opinion Quantification}
\label{current_method}
We use the following definitions from subjective logic to capture second-order uncertainty to design our data opinion quantification model. 

% In the following, the required definitions and concepts in subjective logic are provided:
 
\vspace{.2cm}
\noindent\textbf{Definition 1 (DNN Component).} In a DNN, an input data feature, a model parameter (weight or bias), and/or a neuron are considered as DNN components.

\vspace{.2cm}
\noindent\textbf{Definition 2 (Binomial Opinion).} An opinion about a subjective variable $x$ in a binary domain $\mathbb{X}=\{x,\overline{x}\}$ is a \emph{Binomial Opinion} owned by a DNN component. A binomial opinion~\cite{sl} by the DNN component $v$ about the model performance as the subjective variable, is defined as a tuple of four determinants, % $\hfill \blacksquare$
\begin{equation}
    O_{v} = \{b_{v},d_{v},u_{v},a_{v}\} \quad \text{s.t.} \quad b_v,d_v,u_v,a_v \in \mathbb{R}^{[0, 1]}\ ,
\label{op}
\end{equation}
where $b_v$ denotes the amount of belief in support of the subjective variable $x$, $d_v$ denotes the amount of disbelief about $x$, the uncertainty mass $u_v$ represents the lack of confidence about $x$ caused by vacuity of evidence in a DNN model, and $a_v$ is the prior probability about $x$ without considering any evidence which can take a prior bias measure into account (see Appendix~\ref{sl_app} for more details on opinion determinants). 

Furthermore, these opinion masses should meet the following additivity requirement: 
\begin{equation}
b_v + d_v + u_v = 1\ .
\label{additivity}
\end{equation}

A binomial opinion by the DNN component $v$ is equivalent to a Beta PDF which is a particular binary version of Dirichlet PDF~\cite{sl}. The expected probability of the PDF yields to a projected probability as \emph{trustworthiness} of a DNN component which considers the component's belief and uncertainty about model performance simultaneously as,
% Therefore, a bijective mapping between a binomial opinion and a Beta PDF is defined as the equality between the expected probability of a Beta PDF and the projected probability of a binomial opinion~\cite{sl}. Therefore, this expectation can yield to a projected probability as trustworthiness of a DNN model which is considering the model belief and uncertainty about model performance simultaneously as, 
\begin{equation}
    \text{Projected Probability:} \quad PP_v = b_v + u_v.a_v\ .
\label{projprob}
\end{equation}

Since DNN components impact the model performance in each training epoch, following subjective logic, each DNN component $v$ has a binomial opinion about the DNN model performance. Furthermore, there are binary~\cite{sl} or multi-source operators~\cite{multi1} defined in subjective logic like Multiplication ($\otimes$), Comultiplication ($\widehat{\otimes}$), Average Fusion ($\oplus$), Cumulative Fusion ($\widehat{\oplus}$), and Multi-source Average Fusion ($\bigoplus$), to calculate the output opinion achieved based on a specific relation amongst two or more opinions as operands. 

Based on the the opinion propagation method in~\cite{hope}, for each feature of each data point $d \in \mathcal{D}$ such that $d=\{f_1,f_2,...,f_n\}$ with the true label $y \in Y$, the data feature opinions are initialized by given and equal opinions $O_d = \{O_{f_j}\ |\ j \in \mathbb{N}^{[1,n]}\}$ as feature opinion set and $O_y$ as true label opinion. These data opinions are propagated through the DNN similar to the data forward propagation. In each layer $i$, the forward propagation is fulfilled by the multiplication of the last layer neurons' opinions and the corresponding weight opinions denoted by $O_{w_i}$. For the first hidden layer, the multiplication is done between weight opinions and data opinions. {\color{blue} Thus, each neuron's opinion is dependent to the opinion of weights and the previous layer's neurons.} Then, in the layer $i$, the opinion of each neuron denoted by $O_{N_i}$, is achieved by combining these multiplications together along with the bias opinions denoted by $O_{b_i}$:
\begin{align}
O_{N_1} &= \bigoplus(O_{D} \otimes O_{w_1}, O_{b_1})\ , \\
O_{N_i} &= \bigoplus(O_{N_{i-1}} \otimes O_{w_i}, O_{b_i})\ .
\end{align}

Continuing the forward propagation, the forward opinion of the output neuron $l$ denoted by $O_{\hat{y}}^l$ is achieved. Moreover, each output neuron $l$ has a true label opinion denoted by $O_y^l$ and a backward opinion denoted by $O_{back}^l$ achieved by backward propagation of the opinions. To compute the backward opinion, the error opinion $O_{\delta}^l$ of the output neuron $l$ is computed based on a single threshold $\phi$. Comparing the error $|y-\hat{y}|$ with the threshold $\phi$, the positive or negative evidence are counted to form the belief and disbelief of error opinion, respectively. Then, the backward opinion of the output neuron $l$ is obtained by the multiplication of the target label opinion $O_y^l$ and the error opinion $O_{\delta}^l$:
\begin{equation}
O_{back}^l = O_y^l \otimes O_{\delta}^l\ .
\label{backward_op}
\end{equation}

Then, the opinion of each output neuron $l$, denoted by $O_{out}^l$, is computed as,
\begin{equation}
O_{out}^l = O_{\hat{y}}^l \oplus O_{back}^l\ ,
\label{out_eq}
\end{equation}
where $O_{\hat{y}}^l$ and $O_{back}^l$ are forward opinion and backward opinion respectively. 

Finally, the opinion of the DNN model $\mathcal{M}$ denoted by $O_{\mathcal{M}}$, is obtained by applying average fusion on all output neurons' opinions:
\begin{equation}
O_{\mathcal{M}} = \bigoplus_l(O_{out}^l)\ .
\label{model_eq}
\end{equation}

\vspace{2mm}
\noindent
\textbf{Limitations.} 
%\label{limit}
The opinion propagation method proposed in~\cite{hope} suffers from some limitations:

\noindent
1) The input data opinions are initialized by arbitrary values. For example, all data opinions are initialized either as Max Belief ($b=1$), Max Uncertainty ($u=1$), Neutral Opinions ($b=d=u=\frac{1}{3}$), or Equal Belief and Disbelief ($b=d=0.5$). 
%are considered for input data. 
This limitation prevents capturing opinions that are originated from diverse intrinsic properties of input data features (e.g., skewness or kurtosis of a feature distribution). 

\noindent
2) The assigned opinion tuples can only have different values when the stochastic training process is applied. When a mini-batch training process is applied, the opinion tuples are forced to have the same values, i.e., $O_D = \{O_{f_j}\ |\ \forall j \neq k: O_{f_j}=O_{f_k}\}$. This limitation is originated in the propagation method described in~\cite{hope}, as no algorithm is provided for aggregating opinions in one mini-batch.
%(see Section~\ref{}). 

%where erefore, the propagation method is limited to stochastic training when opinion tuples  initialization process reported in [] is silent about how the   limitatation is 

%with size greater t 1 is  The simplification on assigning same opinion tuples to all input data with arbitrary values has a major implication in the DNN mini-batch training process. When the values are equal, the aggregated values are computed based on a simple computation of average or summation. However, when data points in one mini-batch have diverse values for their tuples, a simple aggregation would lead the tuples are not equal  ion ed values in one mini-batch   In DNN 

%For example,... 
%Therefore, a symmetric and arbitrarily assignment of opinions either incorrectly quantifies the overall model opinion (see limitation 2) or such intrinsic properties need to be discarded in data opinion propagation. 


%1) The input data opinions are initialized by the same opinion tuples with  arbitrary values, i.e.  $O_D = \{O_{f_j}\ |\ \forall j \neq k: O_{f_j}=O_{f_k}\}$. For example, all data opinions are initialized either as Max Belief ($b=1$), Max Uncertainty ($u=1$), Neutral Opinions ($b=d=u=\frac{1}{3}$), or Equal Belief and Disbelief ($b=d=0.5$). 
%are considered for input data. 
%This limitation prevents capturing opinions that are originated from diverse intrinsic properties of input data features (e.g., skewness or kurtoses of a feature distribution). For example,... Therefore, a symmetric and arbitrarily assignment of opinions either incorrectly quantifies the overall model opinion (see limitation 2) or such intrinsic properties need to be discarded in data opinion propagation. 

%propogate the opinions  will limit the propagation process ( tuples leads to  that  leads to loss of As a result, model performance is influenced by the intrinsic properties of input data, therefore, different opinions for input data features should be considered based on the data statistical attributes. 

%2) The simplification on assigning same opinion tuples to all input data with arbitrary values has a major implication in the DNN mini-batch training process. When the values are equal, the aggregated values are computed based on a simple computation of average or summation. However, when data points in one mini-batch have diverse values for their tuples, a simple aggregation would lead the tuples are not equal  ion ed values in one mini-batch   In DNN 

%When the tuples are equal,  the second limitation Mini-batch training is commonly used to train a DNN model and reduces the risk of local minimum, grants a robust convergence, and follows the gradient of the true generalization error~\cite{deeplearning}. An implication of Limitation 1 is that in mini-bacth training, the exploration on how the propagation and aggregation process of mini-batch data opinions is fulfilled, is not completed.
% ....cannot be completed (or not being explored) how the propegation (or agreggation) process will be completed. Thus mini batch training cannot be completed if  opinions are not equal. 
%Note that cheng et al.~\cite{hope} reported that compared to stochastic training (underfitting, i.e. the loss does not decrease), mini-batch training shows advantages in both accuracy and trustworthiness. 
% As the accuracy and the projected probability are directly correlated, the model performance is not converged with only one epoch of stochastic or mini-batch training.
\noindent
3) Following Equation~\ref{backward_op}, error opinion $O_{\delta}^l$ can be computed only based on positive and negative evidence (comparing the error with a single threshold to decide negativity or positivity), while we may encounter conditions where the evidence observed based on $|y-\hat{y}|$ is itself uncertain. Thus the evidence needs to be evaluated not only as positive or negative but also as uncertain. 
%Therefore, the error opinion $O_{\delta}^l$ computation in the current propagation method cannot capture such uncertainty. 

%the computation of error opinion evid are considered during optimization based on a single threshold regardless of considering uncertain observations in model predictions.




% 3) Cheng et al. compare the trustworthiness of the DNN over one epoch of sample by sample training and multiple epochs of mini-batch training until convergence~\cite{hope}. Their results indicated that compared to stochastic training (underfitting, i.e., the loss does not decrease), mini-batch training shows advantages in both accuracy and trustworthiness. As the accuracy and the projected probability trustworthiness are directly correlated, the model performance is not converged with only one epoch of stochastic or mini-batch training. 

% reviewer HEADS UP!
% CVontraray to [], we define the concept of {\color {red} interpretable opinions} as opinions rooted in the ... property of data. explain skewness,..,.,. Then describe and CITE why these properties can be c onsdiered as opinions. THEN why this can solve the limitatiosn you descrtivbed in subsection above.
% THEN descrribe your expanded model
%\vspace{-0.1cm}
\section {Proposed Opinion Propagation Model}
%Using mini-batch training, we consider the distributional and statistical properties of the training data. These intrinsic attributes can impact the model performance. Contrary to~\cite{hope}, 

%In our model we address the three limitations described in section... 


%We define the concept of interpretable opinions for input data as opinions rooted in the statistical properties and distributional information provided by training data points. According to the Definition~\ref{op}, we initialize the determinants of an opinion in each numerical feature based on two different groups of statistical and distributional factors. Each factor is meaningfully considered for initializing one opinion determinant based on its concept and definition which is further discussed in the following. The first group includes Cronbach's Alpha, Data Quality Score (DQS), and Eigenvalue ratio, i.e. explained variance. The second group includes Skewness, Kurtosis, and the distributional Variance. 

%In this research, we quantify interpretable opinions for training data instead of given opinions, and propagate it through the DNN model to consider the impact of input data on the model performance. Furthermore, we improve the methodology of opinion propagation proposed in~\cite{hope} in a way that the opinion optimization is performed based on conventional back propagation algorithm to update the parameters' opinions during training process in a DNN based on some thresholds to identify positive, negative, and uncertain evidence for the corresponding opinion determinants. 

% The input dataset has the feature set $F$ as,
% \[ 
% F=\{\ f_s\ |\ s \in \mathbb{N}^{[1,n_f]}\ \}\ ,
% \] 
% and a set of true labels (target labels) $L$ as,
% \[
% L=\{\ l_s\ |\ s \in \mathbb{N}^{[1,n_l]}\ \}\ , 
% \]
% where $n_f$ and $n_l$ are the total number of features and the total number of labels (classes), respectively.
The input dataset has the feature set $F=\{ f_s\ |\ s \in \mathbb{N}^{[1,n_f]}\}$ and a set of true labels $L=\{ l_s\ |\ s \in \mathbb{N}^{[1,n_l]} \}$ where $n_f$ and $n_l$ are the total number of features and the total number of labels, respectively.
We also define a new concept as \emph{Interpretable Opinions} for input data. Interpretable opinions are opinions that are rooted in the statistical properties of the training data set (e.g., skewness). Therefore, contrary to~\cite{hope}, in our model the data opinions are not required to have given arbitrary values. 

{\color{blue} In a DNN model, each DNN component has its own opinion about the model performance. For each mini-batch of training data, the proposed approach acts as an operator to generate data opinions for each data feature. We first iterate over the features to initialize each feature opinion's determinants due to being a numerical or non-numerical feature as discussed in Section~\ref{op_init_sec}. Then, we can feed the model by the training data along with their interpretable opinions.
}  
% Then, to optimize the parameters' opinions about model performance, we feed and propagate the initial opinions along with the input data to the model for training process according to Section~\ref{current_method}.
% In the next section, we will describe our proposed approach to initialize input data opinions based on their statistical and conceptual properties.

%\vspace{-0.1cm}
\subsection{Data Opinion Initialization}
\label{op_init_sec}
% Uncertainty propagation and quantification are considered prominent in some settings like federated systems which are sensitive in terms of data privacy since clients' personal data may carry a considerable amount of valuable information. 

% As it is stated in~\cite{hope}, the paper used fixed given opinions to initialize data feature opinions regardless of the nature of data features like Max Belief ($b=1$), Max Uncertainty ($u=1$), Neutral Opinions ($b=d=u=\frac{1}{3}$), and Equal Belief and Disbelief ($b=d=0.5$). 

% Moreover, there may be real-world opinions provided for a dataset which are rarely initiated by the experts of its domain. 

% Therefore, specific given initialization values like in~\cite{hope} can give rise to an inaccurate computation in deeper components' opinions in the network. Moreover, the whole process has a general goal to propagate data uncertainty through a network and ultimately, quantify the entire uncertainty of a federated setting. 

% During training process, the model updates its parameters' opinions and outputs its total opinion. Each feature or label opinion individually participates in training process since each neuron's opinion in each layer is produced by the parameters' opinions and the neurons' opinions from the previous layer. Therefore, for opinion propagation through a network, the opinions of data features and labels should be separately initialized. 

% In data preprocessing level, categorical features are encoded to a numerical format by one-hot or ordinal encoding if it is required. Furthermore, we should do imputation for all features after calculating Data Quality Score (DQS) for each feature which will be discussed in the following section. Moreover, before data opinion initialization, the numerical features in the raw dataset will be normalized and scaled by approaches like Maximum Absolute Scaler or MinMax Scaler. 

% Data opinion initialization would have a significant impact on the total model opinion being propagated through a DNN.
We partition the initial dataset into two groups based on their different statistical nature: numerical and categorical/target features. Following Definition~\ref{op}, we initialize the opinion determinants of each numerical feature based on two different groups of statistical and distributional factors. {\color{blue} The selection of these factors to initialize the opinion determinants, i.e., belief, disbelief, and uncertainty, is based on their interpretations and definitions/applications, which are further discussed in the following. The selected factors are different due to the significant difference in the nature of numerical, categorical/target features. Thus, we devise a different feature opinion initialization for each type of features based on different statistical properties in the batch of training data. Therefore, there is flexibility in selecting different statistical measures to initialize interpretable data opinions.}
%Each factor is considered for initializing one opinion determinant based on its concept and definition which is further discussed in the following. 
The first group includes Cronbach's Alpha, Data Quality Score (DQS), and Eigenvalue ratio, i.e., explained variance. The second group includes Skewness, Kurtosis, and the Distributional Variance. 
For categorical features and target labels, we initialize the opinion determinants for each feature based on DQS, Shannon entropy for uncertainty mass, and opinion additivity requirement.

{\color{blue}Base rates for each data feature opinion are the prior probability about the model performance without considering training data points or any evidence.} Since we intend to initialize the input data opinions in an unbiased manner, base rates for each feature as prior probabilities are selected randomly from a normal (Gaussian) distribution $\mathcal{N}(\mu,{\varepsilon}^2)$ where $\mu$ is the mean value and $\varepsilon$ is a relatively small positive constant as the standard deviation. In this research, we select $\mu=0.5$ and $\varepsilon=0.05$ to randomly and equiprobably determine the base rates for belief and disbelief about model performance. 

\vspace{2mm}
\noindent
\textbf{Numerical Features.}
The first determinant of a feature's opinion is the belief $b_{f}$ for the numerical feature $f \in F$ of the training data, which is initialized by the equally-weighted average of two different factors statistically selected to indicate the belief in data as, 
\begin{equation} %^{c,t}
    b_{f} = \frac{1}{2}[\frac{{EV}_{f}}{\sum_{s=1}^{n_f} {EV}_{f}}.{\alpha}_{\mathcal{D}_n} + \exp(-\frac{({skew}_{f})^{\beta_1}}{\beta_2})]\ ,
\label{num_b}    
\end{equation}
where ${EV}_{f}$ is the eigenvalue used for applying explained variance of the feature $f$ extracted from the covariance matrix of feature set, and ${skew}_{f} \in \mathbb{R}$ is the amount of skewness of the numerical feature $f$. 

{\color{blue} As the results of particular measures for each numerical feature $f$ of each mini-batch of training data, e.g., skewness and kurtosis, have an unlimited range of values, we use the exponential function $func: \mathbb{R} \rightarrow \mathbb{R}^{[0,1]}$ to be applied over these statistical measures as,
\begin{equation}
func(z_f)=\exp(-\frac{(z_f)^{\beta_1}}{\beta_2})\ ,
\end{equation}
where $z_f$ is the measure of skewness or kurtosis for the feature $f$.

The function $func$ creates a score function from the skewness and kurtosis of $f$ to be scaled into $\mathbb{R}^{[0,1]}$ with respect to two constants ${\beta}_1 \in \mathbb{R}^+$ and ${\beta}_2 \in \mathbb{R}^+$ as the smoothness factor and the scaling factor, respectively.}

The skewness measure can show how much the distribution of $f$ is biased in one side around the mean value, i.e., how much it is far from an ideal normal distribution without skewness. In skewed data, the tail of the distribution are considered as outliers which negatively impacts the model accuracy. {\color{blue} As in the proposed method, the model trustworthiness is correlated to the model accuracy, the skewness can negatively impact the total model trustworthiness as well.} In addition, ${\alpha}_{\mathcal{D}_n}$ is a metric called \textit{Cronbach's Alpha}~\cite{cronbach} for the numerical dataset $\mathcal{D}_n \subseteq \mathcal{D}$ of the DNN as, 
\begin{equation}
    {\alpha}_{\mathcal{D}_n} = \frac{n_f}{n_f - 1}.\frac{\sum_{i=1}^{n_f}\sum_{j \neq i}^{n_f} {\sigma}_{ij}}{\sum_{i=1}^{n_f}\sum_{j=1}^{n_f} {\sigma}_{ij}}\ ,
\label{cronbach}    
\end{equation}
where ${\sigma}_{ij}$ is the covariance between two features $f_i$ and $f_j$. 

This metric can measure the overall reliability and consistency among various features in a dataset and is partitioned for each feature using their eigenvalue ratios. These eigenvalue ratios are explained variance that can demonstrate the amount of information that each feature carries in a dataset. 

The second determinant of a feature's opinion is the disbelief mass $d_{f}$ for the numerical feature $f \in F$ of the training data and is initialized by the equally-weighted average of two different factors statistically selected to indicate the amount of disbelief in data as,
{\color{red}
\begin{equation}
% \begin{align}
d_{f} = \frac{1}{2}[(1 - {DQS}_{f}) + sigmoid(\frac{{kurt}_{f}}{{\beta}_1} - {\beta}_2)]\ ,
% \label{num_d}
% \end{align}
\end{equation}
}
{\color{blue}
\begin{equation}
% \begin{align}
d_{f} = \frac{1}{2}[(1 - {DQS}_{f}) + (1-\exp(-\frac{({kurt}_{f})^{\beta_1}}{\beta_2}))\ ,
\label{num_d}
% \end{align}
\end{equation}
}
where ${DQS}_{f}$ is \emph{Data Quality Score} for the numerical feature $f$ of the DNN. Furthermore, ${kurt}_{f}$ is the amount of kurtosis of the numerical feature $f$ that indicates the chance of the presence of outliers in the distribution of $f$. In fact, this value amounts to the presence of outliers and can be interpreted as unreliability or disbelief for a distribution. {\color{blue} Therefore, the model trustworthiness which is correlated to the model performance is again negatively influenced by the kurtosis of each data feature.}
{\color{red} Moreover, the function $sigmoid: \mathbb{R} \rightarrow \mathbb{R}^{[0,1]}$ as,
\begin{equation}
sigmoid(\frac{{kurt}_{f}}{{\beta}_1} - {\beta}_2)=\frac{1}{1+\exp(-\frac{{kurt}_{f}}{{\beta}_1} + {\beta}_2)}\ ,
\end{equation}
creates a scaled function from the kurtosis of $f$ with respect to two constants ${\beta}_1 \in \mathbb{R}^+$ and ${\beta}_2 \in \mathbb{R}^+$ as the smoothness factor and the scaling factor, respectively.} 

Before preprocessing the training data, the data quality score is calculated based on the rate of different data issues. We used IBM Data Quality Score\footnote{\url{https://www.ibm.com/docs/en/iis/11.7?topic=results-data-quality-score}} for this purpose. We consider missing or meaningless values (e.g., $NaN$) and data type mismatch as two major data issues for each feature $f$. Our choices of data quality issues are arbitrary and the score can incorporate more or less potential issues. {\color{blue} The data quality can directly impact the model performance since each missing or misleading data value is considered as lack of information which can deviate the model from convergence or being effectively trained. Thus, in the proposed approach, the data quality is directly correlated to the model trustworthiness.}%Furthermore, considering further data issues in calculating data quality score is possible with respect to the presence of various potential issues. 
The data quality score is calculated as follows:
\begin{equation}
{DQS}_{f} = \prod_{i=1}^{N({Issue}_{f})} (1 - R({Issue}_{f}(i)))\ ,
\label{dqs}
\end{equation}
where $N({Issue}_{f})$ is the number of data issues considered for the feature $f$, and $R({Issue}_{f}(i))$ is the rate of $i$th data issue considered for the feature $f$.

The last determinant of a feature's opinion is the uncertainty $u_{f}$ for the numerical feature $f \in F$. The uncertainty is the most prominent determinant since it can be considered as one of the indicators of trust in the system. The uncertainty value is initialized by the equally-weighted average of two different factors statistically selected to indicate the amount of uncertainty in data:
\begin{equation}
u_{f} = \frac{1}{2}[(1 - \frac{{EV}_{f}}{\sum_{s=1}^{n_f} {EV}_{f}}) + {\sigma}_{f}^2]\ ,
\label{num_u}    
\end{equation}
where ${\sigma}_{f}^2$ is the amount of total variance of the feature $f$. 

Since the variance of a distribution in data is interpreted as an indicator of the aleatoric uncertainty (capturing the inherent noise in the data)~\cite{uncer-var}, we exploit the variance as one factor for initialization of uncertainty in the feature opinion. The remaining fraction of the explained variance or eigenvalue ratio of a feature is considered the second factor for uncertainty initialization. Finally, to satisfy the opinion requirement stated in Equation~\ref{additivity}, we apply the softmax function over the opinion determinants already computed.
\vspace{2mm}

% \vspace{2mm}
\noindent
\textbf{Categorical Features and Target Labels.}
%As the nature of categorical and numerical data points are significantly different, we initialize the opinion determinants of each categorical feature and target labels in a different way. 
%The primary determinant of a feature's opinion is the uncertainty $u_{f}$ for the categorical feature $f \in F$ of the DNN which contains $n_s$ distinct categories. In this study, 
For a categorical feature $f \in F$ of the DNN which contains $n_s$ distinct categories and a target label $l \in L$, entropy is exploited as a measure of uncertainty~\cite{uncer5}. We utilize Shannon entropy~\cite{shannon} as an entropy which is widely used in machine learning because of its specific attributes as a function. This uncertainty is initialized as,
\begin{align}
u_{f} &= - \sum_{i=1}^{n_s} \frac{N(c_i)}{m}.\log \frac{N(c_i)}{m}\ , \\
u_{l} &= - \frac{N(l)}{m} \sum_{s=1}^{n_l} \frac{N(l_s)}{m}.\log \frac{N(l_s)}{m}\ ,
\label{cat_u}    
\end{align}
where $N(c_i)$ is the number of the category $c_i$ in the categorical feature $f$, and $m$ is the number of total samples (data points) of the training data. 

% Based on Uniqueness Theorem in~\cite{khinchin} by Khinchin, there is a type of functions $H(P)$ for entropy computation as follows:
% \begin{equation}
% H(P) = H(p_1,p_2,...,p_n) = - \gamma \sum_{i=1}^{n} p_i.\log p_{i}
% \label{uniq}    
% \end{equation}
% \[
% \text{s.t.} \qquad 0 \leq p_i \leq 1 \quad \text{and} \quad \sum_{i=1}^n p_i = 1\ ,
% \]
% where $\gamma$ is a positive real constant.

% Shannon entropy is a specific version of this type of functions ($\gamma=1$ and binary logarithm) with particular attributes, e.g. 1)~continuous, 2)~maximal for uniform distributions, 3)~Non-negative, 4)~additive for independent events, 5)~increasing by the number of outcomes with non-zero probabilities, 6)~zero for certain outcomes, and 7)~permutation-invariant.
% \vspace{-0.05cm}
% \begin{itemize}
%     \item Continuous 
%     \item Maximal for uniform distributions
%     \item Non-negative
%     \item Additive for independent events
%     \item Increasing by the number of outcomes with non-zero probabilities
%     \item Zero for certain outcomes
%     \item Permutation-invariant
% \end{itemize}

% We use the same Shannon function to initialize the uncertainty $u_{l}$ of the target label $l \in L$ based on the following equation:
% \begin{equation}

% \label{label_u}
% \end{equation}

To initialize disbelief for categorical data, we use the following equation based on the same Data Quality Score already computed in numerical feature section as,
\begin{equation}
d_{f} = 1 - {DQS}_{f} \qquad \text{and} \qquad d_{l} = 0\ .
\label{cat_d}    
\end{equation}
% where ${DQS}_{f}$ is Data Quality Score for the categorical feature $f$ of the training data. 
The disbelief for the target labels can be ignored and considered zero since the true labels are constant (ground truth) and do not negatively impact the model performance.

The last determinant is the belief in categorical data features. Following the additivity requirement~\ref{additivity} of binomial opinions, we use the following equations to initialize the belief of the categorical features and target labels as,
\begin{equation}
b_{f} = 1 - u_{f} - d_{f} \qquad \text{and} \qquad b_l = 1 - u_{l}\ .
\label{cat_b}    
\end{equation}

% In general, we demonstrate the entire procedure of opinion initialization for the input data by Algorithm~\ref{algo1}. 

% We also initialize the model parameters opinions using Uncertainty Maximization~\cite{sl} (see Appendix~\ref{parint_app} for more details).
% However, we skip the parameter opinion initialization and feed forward opinion propagation here, and describe them in details in the Appendices~\ref{par_op_init} and~\ref{feed_op}. 
% In Uncertainty Maximization, an opinion (for the parameter $\theta$) should have at least one belief mass of zero and the uncertainty is calculated as,
% \begin{equation}
%     u_{\theta} = \min(1,\frac{PP_{\theta}}{a_{\theta}})\ ,
% \end{equation}
% where $PP_{\theta}$ and $a_{\theta}$ is the projected probability and the base rate of the parameter $\theta$'s opinion, respectively.

{\color{blue}
\subsection{Parameter Opinion Initialization}
\label{par_op_init}
According to Definition 1, the model parameters, i.e., weights and biases, are considered as DNN components in the model. Before training process, we need to initialize the opinions of the model parameters along with initializing the model parameters. Thus, we initialize the parameters' opinions using \emph{Uncertainty Maximization}~\cite{sl} according to the lack (vacuity) of observations since before starting the training process, there are no data points seen by the model. In uncertainty maximization, an opinion for the parameter $\theta$ should have at least one belief mass of zero, i.e., $b_{\theta} = 0$, and the uncertainty is calculated as,
\begin{equation}
    u_{\theta} = \min(1,\frac{PP_{\theta}}{a_{\theta}})\ ,
\end{equation}
where $PP_{\theta}$ and $a_{\theta}$ are the projected probability and the base rate of the parameter $\theta$'s opinion, respectively. 

We exploit randomly generated opinions from the uniform distribution $\mathcal{U}(0,1)$ with respect to the opinion additivity requirement~\ref{additivity} to fairly and uniformly select the opinions' determinants. Then, we apply uncertainty maximization on the generated parameters' opinions to maximize the uncertainty before starting the training phase.   
% (see Appendix~\ref{parint_app} for more details). 
}

\subsection{Opinion Optimization Process}
\label{op_opt}
%In training process, a specific number of training epochs required to minimize the loss function, i.e., the cross entropy function. 
{\color{red} In the optimization process, predicted outputs in the model are produced during training epochs such that having less difference with the actual output.} {\color{blue} In the optimization process, the DNN model is trained by minimizing the difference between predicted softmax probabilities as outputs and the actual hard-labeled outputs.} The amount of absolute difference between actual and predicted outputs, i.e., $|y-\hat{y}|$, is considered as a piece of evidence to form the determinants of error opinion $O_{\delta}$. We need to count the positive and negative evidence as well as considering uncertain evidence in the model output to form the error opinion. Therefore, we split the probabilistic interval $[0,1]$ into three separate partitions based on two thresholds $\phi_1$ and $\phi_2$:
\[
\phi_1 \in \mathbb{R}^{(0,1)} \quad \text{and} \quad \phi_2 \in \mathbb{R}^{(0,1)} \quad \text{s.t.} \quad 0<\phi_1 \leq 0.5 \leq \phi_2< 1 
\]

We compare the absolute error $|y-\hat{y}|$ with the two thresholds $\phi_1$ and $\phi_2$ to form the error opinion's determinants. In each batch of training data, these three partitions are used to count the number of evidence denoted by $r$, $s$, and $t$ to form belief, disbelief, and uncertainty, respectively. Counting the number of evidence for each opinion's determinant is done by categorizing the absolute difference into $\mathbb{R}^{[0,\phi_{1}]}$ ($0 \leq |y-\hat{y}| \leq \phi_1$) as positive evidence to form $r$, $\mathbb{R}^{[\phi_{2},1]}$ ($\phi_2 \leq |y-\hat{y}| \leq 1$) as negative evidence to form $s$, and $\mathbb{R}^{(\phi_{1},\phi_{2})}$ ($\phi_1 < |y-\hat{y}| < \phi_2$) as uncertain evidence to form $t$. Here, the absolute difference is between two probability values, i.e., a softmax result and actual label one-hot encoding. Thus, the minimum and maximum of the difference will be 0 and 1, respectively. These thresholds can be specified arbitrarily with respect to the grid search, and here, we select $\phi_1=0.4$ and $\phi_2=0.7$ {\color{blue}(see Appendix~\ref{thr_app} for more details)}.

For each label $l$ in output neurons, we can build the error opinion $O_{\delta}^l$ based on the observed evidence compared to the thresholds $\phi_{1}$ and $\phi_{2}$ following the equations described in~\cite{sl}:
\begin{equation}
    b_{\delta}^l = \frac{r}{r+s+t} \quad d_{\delta}^l = \frac{s}{r+s+t} \quad u_{\delta}^l = \frac{t}{r+s+t}\ .
\label{makeop}
\end{equation}

We create backward opinion $O_{back}^l$ for each label $l$ by comultiplication between error opinion $O_{\delta}^l$ and true label opinion $O_{y}^l$. {\color{blue} As it is stated in~\cite{sl}, the reason for using comultiplication is to consider the existence (presence) of at least one of the opinion operands in the backward process.} Therefore, we intend to cover and consider the existence of either error opinion or actual opinion or both together: 
\[
O_{\delta}^l = \{b_{\delta}^l,d_{\delta}^l,u_{\delta}^l,a_{\delta}^l\} \quad \text{and} \quad O_{y}^l = \{b_l,d_l,u_l,a_l\} \quad \text{s.t.}
\]
\begin{equation}
    O_{back}^l = O_{\delta}^l\; \widehat{\otimes}\; O_{y}^l\ .
\end{equation}

Then, according to Equations~\ref{out_eq} and~\ref{model_eq}, we take the average fusion of the backward opinion $O_{back}^l$ and forward (predicted) opinion $O_{\hat{y}}^l$ to calculate the output opinion $O_{out}^l$ for each output neuron $l$. Finally, the opinion of the DNN model $\mathcal{M}$ denoted by $O_{\mathcal{M}}$, is obtained by applying average fusion on all output neurons' opinions. {\color{blue} The output opinion results are aggregated by average fusion because these opinions observe each neuron simultaneously (the same process over the same period of time).} 

{\color{blue}To optimize the total model opinion during training, i.e., minimizing the uncertainty while maximizing the belief, we update the opinion of each parameter $\theta$ using the parameter change opinion denoted by $O_{\Delta \theta}$ based on the error opinions. The parameter change opinion is the opinion that is aggregated to the current parameter opinion by cumulative fusion to update it.
%(see Appendix~\ref{opt_app} for more details).

The parameters' opinions $O_\theta$ are updated based on the error value $|y-\hat{y}|$ that the model is achieving during training. Thus, we exploit back propagation approach for the parameters' opinion updates while we are updating the actual parameters in mini-batch gradient descent to minimize the loss function. Based on what we perform in back propagation process to update the parameters in each layer, we initialize the opinions of parameters' change, $O_{\Delta w}$ and $O_{\Delta b}$, backward from the last layer towards the first layer using error opinion set $O_{\delta}$ and its pairwise multiplication with predicted opinion set $O_{\hat{y}}$ in the previous layer. For instance, in layer $i$,
\begin{equation}
    O_{\Delta b}^i = O_{\delta} \qquad \textnormal{and} \qquad O_{\Delta w}^i = O_{\delta} \otimes O_{\hat{y}}^{i-1}\ .
\end{equation}

Finally, in each layer $i$ with the neuron set $layer_i$, we update the error opinion set $O_{\delta}$ by assigning a set consists of multi-source average fusion on the pairwise multiplication of the previous $O_{\delta}$ and the weight opinion set $O_{w}^i$ as follows:
\begin{equation}
    O_{\delta} = \{ \bigoplus_{layer_i}(O_{\delta}\ \otimes\ O_{w}^i)\ \}\ .
\end{equation}

Then, we update the opinion of each parameter $\theta$ denoted by $O_{\theta}$, by taking cumulative fusion on its previous opinion and its change opinion denoted by $O_{\Delta \theta}$, as follows:
\begin{equation}
    O_{\theta} = O_{\theta}\ \widehat{\oplus}\ O_{\Delta \theta}\ .
\end{equation}
}
% \begin{equation}
%     O_{out}^l = O_{back}^l\; \oplus\; O_{\hat{y}}^l\ .
% \end{equation}

% Finally, the opinion of the DNN model $\mathcal{M}$ denoted by $O_{\mathcal{M}}$, is obtained by applying average fusion on all output neurons' opinions:
% \begin{equation}
% O_{\mathcal{M}} = \bigoplus_l(O_{out}^l)\ .
% \end{equation}

% \begin{algorithm}[!ht]  %tbh - H - ht - !htbp
% \DontPrintSemicolon
% \SetInd{0.2em}{1.3em} %Moved vertical bar to the left, default is 0.5 and 1.0
% \SetAlgoLined
% % \SetAlgoNoLine
%   \vspace{1mm}
% %   \KwData{A dataframe $D$ with feature set $F$ and label \\ set $L$ for the client $c$ at the training round$t$}
% % \begin{multicols}{2}
%   \KwInput{\\ \vspace{1mm} $\mathcal{D}$: A dataframe with feature set~$F$ ($F_N \subseteq F$) and label set~$L$;}
%   \vspace{1mm}
%   \KwOutput{Data opinions $O_{\mathcal{D}} = \{\ O_f\ |\ f \in F \cup L\ \}$} 
%     \vspace{2mm}
%     % \tcp{Eq.~\ref{dqs} and~\ref{cronbach}}
%     % \vspace{1mm}
%     $O_{\mathcal{D}} \leftarrow \emptyset$   \tcp*{Data opinions}
%     \vspace{1mm}
%     ${DQS}_{\mathcal{D}} \leftarrow$ Data quality score for features in $\mathcal{D}$ \;
%     \vspace{1mm}
%     $D \leftarrow$ Preprocessing on the dataset $\mathcal{D}$ \;
%     \vspace{1mm}
%     $D_N \leftarrow D_{numerical}$ \;
%     \vspace{1mm}
%     $EV \leftarrow$ Compute eigenvalues of features in $D_N$ \;
%     \vspace{1mm}
%     ${\alpha}_{D_N} \leftarrow$ Compute Cronbach's Alpha of $D_N$ \;
%     \vspace{1mm}
%     ${skew} \leftarrow$ Compute skewness of features in $D_N$ \;
%     \vspace{1mm}
%     ${kurt} \leftarrow$ Compute kurtosis of features in $D_N$ \;
%     \vspace{1mm}
%     \For{\textnormal{each feature} $f \in F$}
%     {
%     \vspace{1mm}
%     $a_{f} \leftarrow $ A random real number from $\mathcal{N}(0.5,{\varepsilon}^2)$ \;
%     \vspace{1mm}
%     % \tcp{Target Labels}
%     \If(\tcp*[f]{Target Labels}){$f$ \textnormal{is target}}
%     { 
%     \vspace{1mm}
%     \For{\textnormal{each label} $l \in L$}
%     {
%     \vspace{1mm}
%     % \tcp{Eq.~\ref{label_u},~\ref{label_d}, and~\ref{label_b}}
%     % \vspace{1mm}
%     $d_l \leftarrow 0$ \;
%     \vspace{1mm}
%     $u_l \leftarrow$ Compute the uncertainty of $l$ \; 
%     \vspace{1mm}
%     $b_l \leftarrow 1 - u_l$ \;
%     \vspace{1mm}
%     $O_l = (b_l,d_l,u_l,a_f)$ \;
%     \vspace{1mm}
%     Appending $O_l$ to $O_{\mathcal{D}}$ \;
%     }
%     }
%     % \tcp*{Numerical Features}
%     \ElseIf(\tcp*[f]{Numerical Features}){$f \in F_N$}       
%     {
%     \vspace{1mm}
%     % \tcp{Eq.~\ref{num_b},~\ref{num_d},~\ref{num_u}, and~\ref{softmax}}
%     % \vspace{1mm}
%     Compute each component of $O_{f}$ \;
%     \vspace{1mm}
%     $b_{f}, d_{f}, u_{f} \leftarrow$ \textit{softmax}($b_{f}, d_{f}, u_{f}$) \;
%     \vspace{1mm}
%     $O_f = (b_f,d_f,u_f,a_f)$ \;
%     \vspace{1mm}
%     Appending $O_{f}$ to $O_{\mathcal{D}}$ \;
%     }
%     % \tcp*{Categorical Features}
%     \Else(\tcp*[f]{Categorical Features})       
%     {
%     \vspace{1mm}
%     \tcp{Eq.~\ref{cat_u},~\ref{cat_d}, and~\ref{cat_b}}
%     \vspace{1mm}
%     $d_{f} \leftarrow 1 - {DQS}_{f}$ \;
%     \vspace{1mm}
%     Compute categories' fraction in $f$ \;
%     \vspace{1mm}
%     $u_{f} \leftarrow$ Compute the uncertainty of $f$ \;
%     \vspace{1mm}
%     $b_{f} \leftarrow 1 - u_{f} - d_{f}$ \;
%     \vspace{1mm}
%     $O_f = (b_f,d_f,u_f,a_f)$ \;
%     \vspace{1mm}
%     Appending $O_{f}$ to $O_{\mathcal{D}}$ \;
%     }
%     }
%     \Return{$O_{\mathcal{D}}$}
% \caption{Data Opinion Initialization}
% \label{algo1}
% % \end{multicols}
% \end{algorithm}

% We demonstrate the entire procedure of parameter opinion update and optimization by the Algorithm~\ref{algo4} in the Appendix~\ref{par_opt}. 
% In the next section, we will describe our experimental results of applying the proposed algorithms on two famous real-world datasets.

% \begin{algorithm}[h!]  %tbh - H - ht - !htbp
% \DontPrintSemicolon
% \SetInd{0.2em}{1.3em} % Moved vertical bar to the left, default is 0.5 and 1.0
% \SetAlgoLined
% % \SetAlgoNoLine
%   \vspace{1mm}
%   \KwInput{\\ \vspace{1mm} $\mathcal{D}_b$: Batch of training data $\mathcal{D}$ in size $b$; \\
%   \vspace{1mm} $P_y^b$ and $P_{\hat{y}}^b$: The actual and predicted output which are $b \times n_l$ matrices for the data batch $\mathcal{D}_b$;\\
%   \vspace{1mm} $n_{\mathcal{M}}$: Total number of layers in the DNN model $\mathcal{M}_\Theta$; \\
% %   \vspace{1mm} $T$: Total number of required training rounds; \\ \vspace{1mm} $C_t$: Finite set of participating clients at the training round~$t$;
%   }
%   \vspace{1mm}
%   \KwOutput{\\ \vspace{1mm} $O_{\mathcal{M}}$: The total opinion for the DNN model~$\mathcal{M}_\Theta$;}
%     \vspace{2mm}
%     \For{\textnormal{each batch} $\mathcal{D}_b$ \textnormal{of the training data} $\mathcal{D}$}
%     {
%     \vspace{1mm}
%     $O_{out}, O_{\delta} \leftarrow \emptyset$ \tcp*{output and error ops}
%     \vspace{1mm}
%     $O_x^F, O_y^L \leftarrow$ \textit{init\_op}($\mathcal{D}_b, y_b$) \tcp*{Algo.~\ref{algo1}}
%     \vspace{1mm}
%     $O_{\hat{y}}^L \leftarrow$ \textit{forward\_op}($O_x^F,O_\Theta^{\mathcal{M}}$) 
%     % \tcp*{Algo.~\ref{algo3}}
%     \vspace{1mm}
%     $\Delta_b^L \leftarrow |P_{\hat{y}}^b - P_y^b|$ \tcp*{$b \times n_l$ matrix}
%     \vspace{1mm}
%     \For{\textnormal{each label} $l \in L$}
%     {
%     \vspace{1mm}
%     $r_b^l, s_b^l, w_b^l \leftarrow$ Number of evidence in $\Delta_b^l \subset \Delta_b^L$ over $(0:\phi_1:\phi_2:1)$ \;
%     \vspace{1mm}
%     $O_e^l \leftarrow$ \textit{make\_op}($r_b^l, s_b^l, w_b^l$) \tcp*{Eq.~\ref{makeop}}
%     \vspace{1mm}
%     $O_{back}^l \leftarrow$ $O_e^l\; \widehat{\otimes}\; O_y^l$ \;
%     \vspace{1mm}
%     Adding $O_{back}^l\; \oplus\; O_{\hat{y}}^l$ to $O_{out}$\;
%     \vspace{1mm}
%     Adding $O_e^l$ to $O_{\delta}$\;
%     }
%     \vspace{1mm}
%     \For{\textnormal{all neurons in the layer} $i=n_{\mathcal{M}}$ \textnormal{to} $1$}
%     {
%     \vspace{1mm}
%     $O_{\Delta b}^i \leftarrow O_{\delta}$ and $O_{\Delta w}^i \leftarrow$ $O_{\delta} \otimes O_{\hat{y}}^{i-1}$ \;
%     \vspace{1mm}
%     $O_{b}^i \leftarrow$ $O_{b}^i \oplus O_{\Delta b}^i$ and $O_{w}^i \leftarrow$ $O_{w}^i \oplus O_{\Delta w}^i$\; 
%     \vspace{1mm}
%     $O_{temp} \leftarrow \emptyset$ \;
%     \vspace{1mm}
%     \For{\textnormal{each neuron in the layer} $i-1$}
%     {
%     $O_{list}^{i-1} \leftarrow \emptyset$ \;
%     \vspace{1mm}
%     \For{\textnormal{each neuron in the layer} $i$}
%     {
%     \vspace{1mm}
%     Adding $O_{\delta}\; \widehat{\otimes}\; O_{w}^i$\ to\ $O_{list}^{i-1}$\;
%     }
%     \vspace{1mm}
%     Adding $\bigoplus(O_{list}^{i-1})$ to $O_{temp}$ \;
%     }
%     \vspace{1mm}
%     $O_{\delta} \leftarrow O_{temp}$ \;
%     }
%     }
%     \vspace{1mm}
%     $O_{\mathcal{M}} \leftarrow$ $\bigoplus(O_{out})$ \tcp*{Average Fusion}
%     \textbf{end} \;
%     \Return{$O_{\mathcal{M}}$}
% \caption{Opinion Optimization Based on Loss Function Using Back Propagation Approach}
% \label{algo4}
% \end{algorithm}



















%   \tcp*{this is a comment}
%   \tcc{Now this is an if...else conditional loop}
%   \If{Condition 1}
%     {
%         Do something    \tcp*{this is another comment}
%         \If{sub-Condition}
%         {Do a lot}
%     }
%     \ElseIf{Condition 2}
%     {
%     	Do Otherwise \;
%         \tcc{Now this is a for loop}
%         \For{sequence}    
%         { 
%         	loop instructions
%         }
%     }
%     \Else
%     {
%     	Do the rest
%     }
    
%     \tcc{Now this is a While loop}
%   \While{Condition}
%   {
%   		Do something\;
%   }


% Informal Definition of Trustworthiness in DNNs: A representation (vectorized, in the form of a multinomial opinion) of how much a user can faithfully trust on the decision-making process of a DNN based on the incorporation of one or more dependent human norms (performance, fairness, privacy, security, transparency, other opinions, and so forth) of trust.
% This definition needs to be formally redefined and determined.
% An extension of this formal definition for decentralized federated settings is needed.

% Assumptions:
% Our assumptions are based on utilizing one or a combination of the following state-of-the-art methods for Secure communications among clients as well as preserving performance (they are thoroughly compatible with our decentralized FL settings using subjective logic): 
% Using local DP in clients for applying a strong privacy-preserving approach,
% Using noisy logits (layers) along with ensemble networks
% Using state-of-the-art adaptive optimization methods such as YellowFin 
% Using secure (distributed) aggregation and shuffling
% Using state-of-the-art Gossip algorithm to reach an average consensus in decentralized FL.

% Taking advantage of spectral gap of our weights matrix.
% ---A complete formalism is in progress, and also, weights and its corresponding matrix should be thoroughly clarified and defined...

% Handling the trade-offs among these criteria is another prominent goal of this research which briefly illustrated in the ensuing subsections. 
% Thus, we skip the basic explanations and trivial descriptions of the related concepts (e.g. training process of Conventional FL).

%------------------------------------------

% As a brief explanation of the whole training process, the conventional Federated Learning architecture consists of a curator (server) located at its centre and coordinates the training activities. Clients or edge devices could practically run into millions in number. These devices communicate at least twice with the server per training iteration. To start with, they each receive the current global model’s weights or gradients from the server, train it on each of their local private data to generate updated parameters which are then uploaded back to the server to be aggregated. After training the same model on various devices with various types of data, their updates (training summary) are sent to the global server, where aggregation of these gradients or weights takes place. After aggregation, the global updates are again sent to the clients, and the training is continuously performed on the client’s device. This process is called a communication round, and many communication rounds happens to further enhance the model's accuracy. This process of communication persists until a preset number of epochs or a certain accuracy is reached. In the ensuing subsections, more details on design and methodology of the proposed research on resolving cold start problem as well as constructing a great practical federated learning system in production which is highly accurate while it is preserving data privacy are illustrated in an approximate order. 

%------------------------------------------

% It is noteworthy that in decentralized FL, the decisions should be taken by the client who is responsible for local learning task, or collaboratively via a consensus procedure \cite{adv}. 
% Moreover, it has good training convergence (highly reduction in the training time) by carefully forming dynamical synchronization gossiping groups as well. 
% Thus, a model segment level decentralized FL is used to tackle this problem. In this way, the load of aggregation process and communication costs between a central server and all the participating clients which exists in Centralized FL model is much more mitigated, and this can cause the bottleneck of communications on certain node to be broken. 

%------------------------------------------

% There are $k$ clients having system heterogeneity as the worst case (rectangular parts of clients with different colors) in the existing federated system which are training on their statistically heterogeneous but compatible data denoted by circular data points with different colors. Then, a local deferentially private approach is applied before their aggregation process in each communication round. Also, they are connected to each other through secure communications which can be certified by approaches such as Secure Multi-party Computation (SMC) \cite{smc} denoted by unidirectional or bidirectional arrows from a client to a subset of its neighbors. Moreover, as the worst case, there is a new client $c_{k+1}$ added to the existing DFL which has heterogeneous and incompatible data denoted by triangular data points with special color, and it is going to be fully synchronized with other participating clients.

% We exploit a novel decentralized FL design called \textit{Segmented Gossip} approach \cite{gossip}, which makes full utilization of node to node bandwidth by transmitting model segmentation in a peer to peer manner with good training convergence. Moreover, we will utilize the approach proposed in \cite{hope} to leverage binomial opinions in output neurons and aggregate them to generate a model's total opinion of $x$ in each round of FL.
% In addition, \textit{FedML} is used to support decentralized topology with directed communication \cite{fedml}. 

%It is remarkable that each client can communicate with a subset of its neighbors. 
% It is remarkable that some clients, as discussed before, do not send messages (communicate) to all of their neighbors; thus, it allows clients to have any arbitrary behavior, and address a specific kind of heterogeneity of edge devices.
%%%% VGG19: 16 convolution layers, 3 Fully Connected layers, 5 MaxPool layers, and 1 SoftMax layer
%Also, models like VGG16 and VGG19 is used in this research as initial models for selected clients. 
% There are other variants of VGG like VGG11, VGG13, and VGG16 which can be leveraged by this study when it is needed. It is worth noting that the process of client selection as a main batch for training process will be based on their availability, temporal changes, and system heterogeneity. 

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.4\linewidth]{rp.PNG}
%     \caption{A general schema of Decentralized Federated Learning encountering a new heterogeneous client.}
%     \label{fig:dfl}
% \end{figure}

% Figure~\ref{fig:dfl} illustrates a typical DFL model where $k$ clients have system and statistical heterogeneity with compatible (synchronized) data in a related scope.