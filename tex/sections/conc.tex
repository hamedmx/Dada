\section{Conclusion}
\label{conc}
% In this paper, we investigated the distributional changes in input data features can impact the model performance. we initialized the training data opinions to be propagated through the DNN using statistical and distributional properties of input data. Moreover, we improved the opinion optimization process by considering positive and negative evidence as well as uncertain evidence in model predictive outputs. We evaluated our proposed method using two real-world datasets, SUSY and KDD CUP 99, containing numerical and categorical features which are considerably different in terms of many statistical, natural, and domain aspects. Therefore, we observed that by decreasing loss and increasing accuracy during our training epochs, the amount of belief and trustworthiness (projected probability) are increasing while our model uncertainty is decreasing significantly. We also demonstrate the impact of different statistical or conceptual factors of data opinion initialization over total model opinion’s determinants. As a future work, we are currently working on propagating data and model opinions through a decentralized federated settings in which different client models are trained by their local data to achieve an improved global model. 
% We intend to quantify the total uncertainty of a federated network based on propagated clients’ opinions over a specific amount of training rounds. Therefore, we utilize our approach in each communication, aggregation, and propagation activities in federated learning without a significant computational cost.
In this paper, we investigated the limitations of the state-of-the-art opinion propagation method to quantify model trustworthiness. We proposed a new approach to initialize input data opinions by interpretable statistical metrics based on intrinsic properties of training data. To initialize the numerical, categorical, and target features, we selected distinct statistical properties originated in input data feature distributions. We also improved the opinion optimization process by considering uncertain evidence in forming the error opinion. Finally, we demonstrated the impact of input data distribution on the model performance and evaluated our proposed data opinion propagation method over two real-world datasets. Our experimental results confirmed that the model trustworthiness and performance are correlated such that the model belief and uncertainty are improved during the training process with respect to different data opinion initialization.   















% In this research, we utilized SL to propagate and measure the uncertainty in a decentralized federated learning system. This method is adequately flexible in different DNN architectures since it acts as an add-on in a ML system by being applied along with the training process in each DNN, therefore, it has no significant additional computational burden. It is also compatible with the state-of-the-art methods of communication amongst different DNNs.

% In this research, We leverage Subjective Logic as a tool and inject it to deep neural networks in order to propagate and measure the uncertainty in a single DNN, with an aim of propagating and quantifying total uncertainty in a decentralized federated learning system. Furthermore, using subjective logic in our proposed approach is fully beneficial and adequately flexible in different DNN architectures since it can act as an add-on in a ML system by being applied along with the training process in each DNN, therefore, it has no significant additional computational burden. It is also compatible with the state-of-the-art methods of communication amongst different DNNs. The objective of our research is to develop a novel method for propagating uncertainty associated with training dataset as subjective opinions through a DNN model leading to the total model opinion containing total model uncertainty. Thus, we exploit some statistical and conceptual method to quantify and initialize data uncertainty and opinions to be propagated based on the nature of input data features. During training process, the model updates its parameters' opinions and outputs its total opinion. Each feature or label opinion individually participates in training process. Therefore, for opinion propagation through a network, the opinions of data features and labels are separately initialized. 
% We evaluate our proposed method using two real-world datasets, SUSY and KDD CUP 99, containing numerical and categorical features which are considerably different in terms of many statistical, natural, inferential, and domain aspects. Based on our evaluation, we expect a DNN model with higher accuracy indicates lower uncertainty. Therefore, we observe that by decreasing loss and increasing accuracy during our training epochs which is the sign of successful learning, the amount of belief is increasing while our model uncertainty is decreasing significantly. We show that the belief and uncertainty will be improved in a DNN model based on its loss function optimization during training process. Moreover, we indicate the impact of successful training and loss optimization on the total opinion and its determinants in the learning model. We also demonstrate the impact of different statistical or conceptual factors of data opinion initialization over total model opinion’s determinants like belief and uncertainty in a way that each opinion determinant is relatively improved during training epochs. As a future work, we are currently working on propagating data and model opinions through a decentralized federated settings in which we can train different client models over their private data without sharing confidential data information to achieve an improved global model. We intend to quantify the total uncertainty of a federated network based on propagated clients’ opinions over a specific amount of training rounds. Therefore, we utilize our approach in each communication, aggregation, and propagation activities in federated learning without a significant computational cost.



